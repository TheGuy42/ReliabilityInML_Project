{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch import nn\n",
    "import pickle\n",
    "import textwrap\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
    "\n",
    "import confseq\n",
    "from confseq import predmix, conjmix_bounded, betting\n",
    "\n",
    "from Energy_ds.dataset import DataPrep, DataModule, EnergyDataset\n",
    "from Energy_ds.config import SEASON, REGION, DatasetConfig\n",
    "\n",
    "from neural.train import LightningTrainer\n",
    "from neural.module import LightningModel, LightningWrapper\n",
    "from neural.mlp import MlpBlock, RMLP, MLPLayer\n",
    "from neural.config import TrainConfig\n",
    "\n",
    "import risk\n",
    "from risk import Risk\n",
    "from algorithm import Hypothesis, ConfSeq\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubplotGenerator:\n",
    "    def __init__(self, rows, columns, dpi=200, **kwargs):\n",
    "        self.fig, self.axes = plt.subplots(rows, columns, dpi=dpi, **kwargs)\n",
    "        self.rows = rows\n",
    "        self.columns = columns\n",
    "        self.current_row = 0\n",
    "        self.current_col = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.current_row = 0\n",
    "        self.current_col = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self) -> plt.Axes:\n",
    "        if self.current_row >= self.rows:\n",
    "            raise StopIteration\n",
    "        ax = self.axes[self.current_row, self.current_col]\n",
    "        self.current_col += 1\n",
    "        if self.current_col >= self.columns:\n",
    "            self.current_col = 0\n",
    "            self.current_row += 1\n",
    "        return ax\n",
    "    \n",
    "    @property\n",
    "    def current_ax(self) -> plt.Axes:\n",
    "        return self.axes[self.current_row, self.current_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoffConf(ConfSeq):\n",
    "    def __init__(self, conf_lvl: float, min_val=None, max_val=None):\n",
    "        super().__init__(conf_lvl, min_val, max_val)\n",
    "    \n",
    "    def calculate_cs(self, x):\n",
    "        # min_val, max_val =  x.min(), x.max()\n",
    "        # normalized = (x - min_val) / (max_val - min_val)\n",
    "        lower_cs, higher_cs = predmix.predmix_hoeffding_cs(x, self.conf_lvl)\n",
    "\n",
    "        # lower_cs = lower_cs * (max_val - min_val) + min_val\n",
    "        # higher_cs = higher_cs * (max_val - min_val) + min_val\n",
    "        return lower_cs, higher_cs\n",
    "    \n",
    "    @staticmethod\n",
    "    def standardise(x):\n",
    "        return (x - x.min()) / (x.max() - x.min())\n",
    "    \n",
    "\n",
    "class EmpbernConf(ConfSeq):\n",
    "    def __init__(self, conf_lvl: float, min_val=None, max_val=None):\n",
    "        super().__init__(conf_lvl, min_val=min_val, max_val=max_val)\n",
    "    \n",
    "    def calculate_cs(self, x):\n",
    "        # min_val, max_val =  x.min(), x.max()\n",
    "        # normalized = (x - min_val) / (max_val - min_val)\n",
    "\n",
    "        lower_cs, higher_cs = predmix.predmix_empbern_twosided_cs(x, self.conf_lvl, running_intersection=False)\n",
    "\n",
    "        # lower_cs = lower_cs * (max_val - min_val) + min_val\n",
    "        # higher_cs = higher_cs * (max_val - min_val) + min_val\n",
    "        return lower_cs, higher_cs\n",
    "    \n",
    "\n",
    "class EmpbernConjmix(ConfSeq):\n",
    "    def __init__(self, conf_lvl: float, min_val=None, max_val=None):\n",
    "        super().__init__(conf_lvl, min_val, max_val)\n",
    "    \n",
    "    def calculate_cs(self, x):\n",
    "        # min_val, max_val =  x.min(), x.max()\n",
    "        # normalized = (x - min_val) / (max_val - min_val)\n",
    "        lower_cs, higher_cs = conjmix_bounded.conjmix_empbern_twosided_cs(x, 1/12, self.conf_lvl)\n",
    "\n",
    "        # lower_cs = lower_cs * (max_val - min_val) + min_val\n",
    "        # higher_cs = higher_cs * (max_val - min_val) + min_val\n",
    "        return lower_cs, higher_cs\n",
    "\n",
    "class EmpbernBetting(ConfSeq):\n",
    "    def __init__(self, conf_lvl: float, min_val=None, max_val=None):\n",
    "        super().__init__(conf_lvl, min_val, max_val)\n",
    "    \n",
    "    def calculate_cs(self, x):\n",
    "        # min_val, max_val =  x.min(), x.max()\n",
    "        # normalized = (x - min_val) / (max_val - min_val)\n",
    "        lower_cs, higher_cs = betting.betting_cs(x, self.conf_lvl)\n",
    "\n",
    "        # lower_cs = lower_cs * (max_val - min_val) + min_val\n",
    "        # higher_cs = higher_cs * (max_val - min_val) + min_val\n",
    "        return lower_cs, higher_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H0(Hypothesis):\n",
    "    def __init__(self, tolerance:float, lower_bound:ConfSeq, upper_bound:ConfSeq):\n",
    "        super().__init__(tolerance, lower_bound, upper_bound)\n",
    "    \n",
    "    @property\n",
    "    def source_upper(self):\n",
    "        return self.source_upper_cs[-1] * self.tolerance\n",
    "    \n",
    "    @property\n",
    "    def target_lower(self):\n",
    "        return self.target_lower_cs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeployedModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: RandomForestRegressor,\n",
    "        hyp_tests: list[tuple[Hypothesis, Risk]],\n",
    "        past:int=np.inf\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.hyp_tests = hyp_tests\n",
    "        self.past = past\n",
    "        \n",
    "        self.input_seq = None\n",
    "        self.label_seq = None\n",
    "        self.pred_seq = None\n",
    "\n",
    "    def update(self, features:np.ndarray, labels: np.ndarray) -> bool:\n",
    "        # features = features.reshape(-1, 1)\n",
    "        preds = self.model.predict(features)\n",
    "        # update input, label and prediction sequences\n",
    "        self.input_seq = self._append_seq(features, self.input_seq)\n",
    "        self.label_seq = self._append_seq(labels, self.label_seq)\n",
    "        self.pred_seq = self._append_seq(preds, self.pred_seq)\n",
    "        # update targed confidence bounds\n",
    "        test_results = []\n",
    "        for hyp, risk in self.hyp_tests:\n",
    "            tr_input, tr_label, tr_pred, = self.input_seq, self.label_seq, self.pred_seq\n",
    "            res = hyp.test(risk(tr_input, tr_label, tr_pred))\n",
    "            test_results.append(res)\n",
    "        return all(test_results)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # fit model\n",
    "        self.model = self.model.fit(x, y)\n",
    "        # calibrate source confidence bounds\n",
    "        preds = self.model.predict(x)\n",
    "        for hyp, risk in self.hyp_tests:\n",
    "            risk_list = []\n",
    "            for i in range(len(x)):\n",
    "                risk_list.append(risk(x[:i+1], y[:i+1], preds[:i+1]))\n",
    "            \n",
    "            min_val, max_val = self.infer_bounds(hyp, risk_seq=np.asanyarray(risk_list), gamma=5, set_bounds=True)\n",
    "            print(f\"Inferred {risk} Bounds: min_val: {min_val}, max_val: {max_val}\")\n",
    "            hyp.calc_source_upper_cs(np.asanyarray(risk_list))\n",
    "        \n",
    "    def to_dataframe(self):\n",
    "        results = []\n",
    "        for hyp, risk in self.hyp_tests:\n",
    "            res = hyp.to_dataframe()\n",
    "            # res['GT'] = self.label_seq\n",
    "            res['risk'] = risk.__class__.__name__\n",
    "            res['hyp'] = hyp.__class__.__name__\n",
    "            results.append(res)\n",
    "        return pd.concat(results, axis=0, ignore_index=True)\n",
    "    \n",
    "    def _append_seq(self, x:np.ndarray|float, seq:np.ndarray, dim:int=0) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        append the input x to the sequence.\n",
    "        parameters:\n",
    "            x (np.ndarray): The input array of shape (n_samples) or a float if x is a single sample.\n",
    "            seq (np.ndarray): The sequence to append to.\n",
    "        \"\"\"\n",
    "        if isinstance(x, float):\n",
    "            x = np.asanyarray([x])\n",
    "        \n",
    "        if seq is None:\n",
    "            return x\n",
    "\n",
    "        new_seq = np.concatenate((seq, x), axis=dim)\n",
    "        return new_seq\n",
    "\n",
    "    def reset(self, source:bool=True, target:bool=True):\n",
    "        if target:\n",
    "            self.input_seq = None\n",
    "            self.label_seq = None\n",
    "            self.pred_seq = None\n",
    "        for hyp, risk in self.hyp_tests:\n",
    "            hyp.reset(source=source, target=target)\n",
    "\n",
    "    def infer_bounds(self, hyp:Hypothesis, risk_seq:np.ndarray, gamma:float=2, set_bounds:bool=False):\n",
    "        std = (risk_seq).std()\n",
    "\n",
    "        min_val = risk_seq.min() - std * gamma\n",
    "        min_val = max(min_val, 0)\n",
    "        # min_val = min(min_val * gamma, min_val / gamma)\n",
    "        max_val = risk_seq.max() + std * gamma\n",
    "        # max_val = max(max_val * gamma, max_val / gamma)\n",
    "\n",
    "        if set_bounds:\n",
    "            hyp.set_bounds(min_val, max_val)\n",
    "        \n",
    "        return min_val, max_val\n",
    "    \n",
    "    def seq_window(self, *seq:np.ndarray) -> tuple[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Get the sequence of the last n samples.\n",
    "        \"\"\"\n",
    "        seq_list = []\n",
    "        for s in seq:\n",
    "            past = max(len(s) - self.past, 0)\n",
    "            seq_list.append(s[past:])\n",
    "            \n",
    "        return seq_list\n",
    "    \n",
    "    def save(self, path:str, reset_source:bool=True, reset_target:bool=True):\n",
    "        self.reset(source=reset_source, target=reset_target)\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(path:str) -> 'DeployedModel':\n",
    "        with open(path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        # train_ds,\n",
    "        # test_ds,\n",
    "        test_batch_size=1,\n",
    "    ):\n",
    "        self.model: DeployedModel = model\n",
    "        # self.train_ds: EnergyDataset = train_ds\n",
    "        # self.test_ds: EnergyDataset = test_ds\n",
    "        self.test_batch_size: int = test_batch_size\n",
    "\n",
    "    def train(self, train_ds: EnergyDataset):\n",
    "        self.model.fit(train_ds.features, train_ds.labels)\n",
    "    \n",
    "    def eval(self, val_ds: EnergyDataset):\n",
    "        preds = self.model.predict(val_ds.features)\n",
    "        mse = sk.metrics.mean_squared_error(val_ds.labels, preds)\n",
    "        mae = sk.metrics.mean_absolute_error(val_ds.labels, preds)\n",
    "        return mse, mae\n",
    "\n",
    "    def test(self, test_ds: EnergyDataset):\n",
    "        ds_len = len(test_ds)\n",
    "        for i in range(0, ds_len, self.test_batch_size):\n",
    "            # prepare batch\n",
    "            batch_end = min(i+self.test_batch_size, ds_len)\n",
    "            x = test_ds.features[i:batch_end]\n",
    "            y = test_ds.labels[i:batch_end]\n",
    "            # print(f\"batch {i} :: x.shape={x.shape}, y.shape={y.shape}\")\n",
    "            # predict\n",
    "            result = self.model.update(x, y)\n",
    "\n",
    "        return self.model.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Hourly_Energy_Consumption/AEP_hourly.csv\"\n",
    "all_paths = ['Hourly_Energy_Consumption/AEP_hourly.csv',\n",
    "            'Hourly_Energy_Consumption/COMED_hourly.csv',\n",
    "            'Hourly_Energy_Consumption/DAYTON_hourly.csv',\n",
    "            'Hourly_Energy_Consumption/DEOK_hourly.csv',\n",
    "            'Hourly_Energy_Consumption/DOM_hourly.csv',\n",
    "            'Hourly_Energy_Consumption/DUQ_hourly.csv',\n",
    "            'Hourly_Energy_Consumption/EKPC_hourly.csv',\n",
    "            'Hourly_Energy_Consumption/FE_hourly.csv',\n",
    "            # 'Hourly_Energy_Consumption/NI_hourly.csv',\n",
    "            'Hourly_Energy_Consumption/PJME_hourly.csv',\n",
    "            'Hourly_Energy_Consumption/PJMW_hourly.csv',\n",
    "            # 'Hourly_Energy_Consumption/PJM_Load_hourly.csv',\n",
    "            # 'Hourly_Energy_Consumption/est_hourly.paruqet',\n",
    "            # 'Hourly_Energy_Consumption/pjm_hourly_est.csv'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = DataPrep(*all_paths)\n",
    "data = prep.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Region'] = data['Region'].apply(lambda x: REGION(x).name)\n",
    "data['Epoch'] = data['Datetime'].apply(lambda x: x.timestamp())\n",
    "grouped = data.groupby(by=['Region']).agg(['min', 'max', 'mean', 'std'])\n",
    "grouped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = sns.relplot(data=data, x='Datetime', y='MW', hue='Region', kind='line', aspect=3, linewidth = 0.8)\n",
    "# g.figure.suptitle(template=f\"Energy Consumption in each region, by season\")\n",
    "# g.figure.set_dpi(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dataset Visualization](/home/guycoh/new/ReliabilityInML_Project/imgs/dataset_vis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = DatasetConfig(\n",
    "    past_hours=[2,3,6, 12, 24, 36],\n",
    "    years=(2006, 2014),\n",
    "    # region=[REGION.AEP, REGION.COMED, REGION.NI],\n",
    "    season=None,\n",
    ")\n",
    "# test_config = DatasetConfig(\n",
    "#     past_hours=[-1, -12, -24, -36],\n",
    "#     years=(2014, 2019),\n",
    "#     region=[REGION.AEP],\n",
    "#     season=SEASON.WINTER,\n",
    "# )\n",
    "# test_config_ood = DatasetConfig(\n",
    "#     past_hours=[-1, -12, -24, -36],\n",
    "#     years=(2011, 2013),\n",
    "#     region=[REGION.AEP],\n",
    "#     season=SEASON.SUMMER,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = EnergyDataset(all_paths, train_config)\n",
    "# test_ds = EnergyDataset(all_paths, test_config)\n",
    "# test_ds_ood = EnergyDataset(all_paths, test_config_ood)\n",
    "\n",
    "columns = train_ds.features_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=150,\n",
    "    min_samples_split=0.05,\n",
    "    min_samples_leaf=0.05,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# rf = GradientBoostingRegressor(\n",
    "#     loss='squared_error',\n",
    "#     learning_rate=0.1,\n",
    "#     n_estimators=100,\n",
    "#     min_samples_split=0.05,\n",
    "#     min_samples_leaf=0.05,\n",
    "#     max_depth=5,\n",
    "#     random_state=42,\n",
    "#     # n_iter_no_change=5,\n",
    "#     verbose=1,\n",
    "# )\n",
    "\n",
    "# rf = AdaBoostRegressor(\n",
    "#     estimator=rf,\n",
    "#     n_estimators=100,\n",
    "#     learning_rate=1,\n",
    "#     loss='square',\n",
    "#     random_state=42,\n",
    "# )\n",
    "\n",
    "rf = ExtraTreesRegressor(\n",
    "    n_estimators=250,\n",
    "    min_samples_split=0.05,\n",
    "    min_samples_leaf=0.05,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# rf = MLPRegressor(\n",
    "#     hidden_layer_sizes=(50, 50, 25),\n",
    "#     activation='relu',\n",
    "#     learning_rate='adaptive',\n",
    "#     max_iter=100,\n",
    "#     shuffle=False,\n",
    "#     random_state=42,\n",
    "#     verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = []\n",
    "\n",
    "min_val = 0 #train_ds.labels.min()\n",
    "max_val = train_ds.labels.max() * 2\n",
    "\n",
    "\n",
    "mae_risk = risk.MAE()\n",
    "hyp_test_1 = H0(tolerance=1.1,\n",
    "                lower_bound=EmpbernConjmix(0.05),\n",
    "                upper_bound=EmpbernConjmix(0.05)\n",
    "                )\n",
    "tests.append((hyp_test_1, mae_risk))\n",
    "\n",
    "quantile_risk = risk.Quantile(0.8)\n",
    "hyp_test_2 = H0(tolerance=1.1,\n",
    "                lower_bound=EmpbernConjmix(0.05),\n",
    "                upper_bound=EmpbernConjmix(0.05)\n",
    "                )\n",
    "# tests.append((hyp_test_2, quantile_risk))\n",
    "\n",
    "for season in SEASON:\n",
    "    mae = risk.MAE()\n",
    "    filter_risk = risk.RiskFilter(mae, columns, lambda df: df['Season']==season.value, name=season.name)\n",
    "    hyp_test_3 = H0(tolerance=1.1,\n",
    "                    lower_bound=EmpbernConjmix(0.05),\n",
    "                    upper_bound=EmpbernConjmix(0.05)\n",
    "                    )\n",
    "    tests.append((hyp_test_3, filter_risk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeployedModel(rf, tests, past=24*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(model, test_batch_size=24*7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.model.reset(source=True, target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.train(train_ds)\n",
    "experiment.eval(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show feature inportances\n",
    "feature_cols = train_ds.features_df.columns\n",
    "featureimportances = pd.DataFrame(experiment.model.model.feature_importances_.reshape(1,-1), columns=feature_cols)\n",
    "# fig, ax = plt.subplots(dpi=200, figsize=(10, 5))\n",
    "g = sns.catplot(data=featureimportances, aspect=2)\n",
    "g.set_xticklabels(rotation=90, fontsize=8)\n",
    "g.figure.suptitle(f\"Feature Importances in the {experiment.model.model.__class__.__name} Model\")\n",
    "g.set_ylabels(\"Importance\")\n",
    "g.set(ylim=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(experiment:Experiment, ds:EnergyDataset, **kwargs):\n",
    "    config = ds.config\n",
    "    experiment.model.reset(source=False, target=True)\n",
    "    result = experiment.test(ds)\n",
    "\n",
    "    for hyp, test_risk in experiment.model.hyp_tests:\n",
    "        # ax = ax_gen.__next__()\n",
    "        g = hyp.plot(layout='constrained', **kwargs)\n",
    "        title = f\"Tolarance: {hyp.tolerance}, Risk={test_risk}, {config}\"\n",
    "        g.set_title(textwrap.fill(title, width=70), fontsize=8)\n",
    "        g.legend(fontsize=8)  # Adjust legend font size and position\n",
    "\n",
    "        g.autoscale(enable=True, axis='both')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = run_experiment(experiment, train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cfg = DatasetConfig(**train_config.__dict__)\n",
    "test_cfg.region = [REGION.DOM]\n",
    "# test_cfg.years = (2014, 2019)\n",
    "test_ds = EnergyDataset(all_paths, test_cfg)\n",
    "test_result = run_experiment(experiment, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment.model.hyp_tests[0][0].plot()\n",
    "# ax_gen_list = [SubplotGenerator(3, 2) for _ in range(len(model.hyp_tests))]\n",
    "print(\"Train Config:\")\n",
    "print(train_config.__dict__)\n",
    "for season in list(SEASON):\n",
    "    config = DatasetConfig(**train_config.__dict__)\n",
    "    config.season = [season] if season is not None else None\n",
    "    # config.years = (2014, 2019)\n",
    "    print(\"Test Config:\")\n",
    "    print(config)\n",
    "    dataset = EnergyDataset(all_paths, config)\n",
    "    print(dataset.features_df.shape)\n",
    "    result = run_experiment(experiment, dataset)\n",
    "    plt.show()\n",
    "    # result = experiment.test(dataset)\n",
    "    # for hyp, test_risk in experiment.model.hyp_tests:\n",
    "    #     # ax = ax_gen.__next__()\n",
    "    #     g = hyp.plot(layout='constrained')\n",
    "    #     g.set_title(f\"Tolarance: {hyp.tolerance}, Risk={test_risk}, Season={config.season}\", fontsize=10)\n",
    "    #     g.legend(fontsize=8)  # Adjust legend font size and position\n",
    "\n",
    "    #     g.autoscale(enable=True, axis='both')\n",
    "    #     plt.show()\n",
    "    \n",
    "    # experiment.model.reset(source=False, target=True)\n",
    "\n",
    "# plt.tight_layout(pad=2,rect=[0.1, 0.1, 0.9, 0.9])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.model.reset(source=False, target=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = TrainConfig(\n",
    "        # loss_fn = 'bce', # str\n",
    "        # optimizer = 'adam', # str\n",
    "        device = f\"gpu\", # str\n",
    "        log = False, # bool\n",
    "        logs_dir = \"logs/\", # str\n",
    "        num_epochs = 30, # int\n",
    "        checkpoints = 'test.pt', # str\n",
    "        early_stopping = None, # int\n",
    "        log_every = 1, # int\n",
    "        timeout = \"00:12:00:00\", # int\n",
    "        # learning_rate = 0.001, # float\n",
    "        # weight_decay = 1e-06, # float\n",
    "        batch_size = 2048, # int\n",
    "        shuffle = False, # bool\n",
    "        num_workers = 9, # int\n",
    "        # train_test_split = 0.5, # float\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Hourly_Energy_Consumption/AEP_hourly.csv\"\n",
    "past_hours = 2\n",
    "\n",
    "prper = DataPrep(path)\n",
    "data = prper.data\n",
    "\n",
    "# train_years = range(2004, 2016)\n",
    "# val_years = range(2016, 2018)\n",
    "\n",
    "def year_cond(start:int, end:int):\n",
    "    return lambda data: data['Year'].between(start, end)\n",
    "\n",
    "def season_cond(season:SEASON):\n",
    "    return lambda data: data['Season'] == season.value\n",
    "\n",
    "def month_cond(month:int):\n",
    "    return lambda data: data['Month'] == month\n",
    "\n",
    "def cond_and(conds:list):\n",
    "    def cond(data):\n",
    "        mask = pd.Series(len(data)*[True])\n",
    "        for c in conds:\n",
    "            mask = mask & c(data)\n",
    "        return mask\n",
    "    # return lambda data: ([cond(data) for cond in conds])\n",
    "    return cond\n",
    "\n",
    "train_conds = [year_cond(2006, 2008), season_cond(SEASON.WINTER)]\n",
    "val_conds = [year_cond(2009, 2012), season_cond(SEASON.SUMMER)]\n",
    "\n",
    "train_ds = EnergyDataset(path, past_hours=past_hours, condition=cond_and(train_conds))\n",
    "val_ds = EnergyDataset(path, past_hours=past_hours, condition=cond_and(val_conds))\n",
    "\n",
    "\n",
    "datamodule = DataModule(train_ds,\n",
    "                        val_ds,\n",
    "                        batch_size=settings.batch_size,\n",
    "                        num_workers=settings.num_workers,\n",
    "                        shuffle=settings.shuffle,\n",
    "                        seed=42,\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datamodule.prepare_data()\n",
    "dl = datamodule.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, label = next(iter(dl))\n",
    "n_features = batch.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.shape, label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class ResidMLP(LightningModel):\n",
    "    def __init__(self, input_size, hidden_size, output_size=1, loss:nn.Module=nn.L1Loss()):\n",
    "        super().__init__(loss)\n",
    "        # self.f = nn.TransformerEncoderLayer(input_size,\n",
    "        #                                     nhead=4,\n",
    "        #                                     dim_feedforward=hidden_size[-1],\n",
    "        #                                     dropout=0.1,\n",
    "        #                                     activation='relu',\n",
    "        #                                     batch_first=True)\n",
    "        \n",
    "        # self.rmlp = RMLP(\n",
    "        #     in_dim=input_size,\n",
    "        #     block_in_dim=hidden_size[0], \n",
    "        #     block_dims=hidden_size,\n",
    "        #     block_nonlins=[nn.ReLU()]*len(hidden_size),\n",
    "        #     n_blocks=2,\n",
    "        #     out_dim=output_size,\n",
    "        #     out_nonlin=nn.Identity(),\n",
    "        #     batch_norm=False,\n",
    "        #     )\n",
    "        # self.mlp = MlpBlock(\n",
    "        #     in_dim=input_size,\n",
    "        #     dims=hidden_size,\n",
    "        #     nonlins=[nn.Tanh()]*len(hidden_size),\n",
    "        #     batch_norm=False,\n",
    "        #     )\n",
    "        self.out_layer = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = x.flatten(1).to(torch.float32)\n",
    "        # x = self.rmlp(x)\n",
    "        # x = self.mlp(x)\n",
    "        # x = self.f(x)\n",
    "        return self.out_layer(x)#.squeeze(-1)\n",
    "    \n",
    "    def accuracy(self, preds: Tensor, labels: Tensor) -> Tensor:\n",
    "        return (preds - labels).abs().mean()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size = n_features#*past_hours\n",
    "rmlp = ResidMLP(input_size=in_size, hidden_size=[24, 16], output_size=1)\n",
    "\n",
    "model = LightningWrapper(rmlp)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = LightningTrainer(settings, \"global_wheat_1\")\n",
    "train_dl, val_dl = datamodule.train_dataloader(), datamodule.val_dataloader()\n",
    "trainer.fit(model, dl_train=train_dl, dl_test=val_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = rf.fit(train_ds.features, train_ds.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(preds, labels):\n",
    "    return np.abs(preds - labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = mae(rf.predict(train_ds.features), train_ds.labels.numpy())\n",
    "val_scores = mae(rf.predict(val_ds.features), val_ds.labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate(model:RandomForestRegressor, val_dl):\n",
    "    scores = []\n",
    "    for batch, label in val_dl:\n",
    "        score = np.abs(model.predict(batch.numpy()) - label.numpy()).flatten()\n",
    "        scores.append(score)\n",
    "    return np.concatenate(scores)\n",
    "\n",
    "train_scores = validate(rf, train_dl)\n",
    "val_scores = validate(rf, val_dl)\n",
    "# scores = []\n",
    "# for batch, label in val_dl:\n",
    "#     score = np.abs(rf.predict(batch.numpy()) - label.numpy()).mean()\n",
    "#     scores.append(score)\n",
    "# print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train_score = {train_scores.mean()}\")\n",
    "print(f\"val_score = {val_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import confseq\n",
    "from confseq import predmix, conjmix_bounded, betting\n",
    "\n",
    "from Energy_ds.dataset import DataPrep, DataModule, EnergyDataset\n",
    "from Energy_ds.config import SEASON, REGION, DatasetConfig\n",
    "\n",
    "from neural.train import LightningTrainer\n",
    "from neural.module import LightningModel, LightningWrapper\n",
    "from neural.mlp import MlpBlock, RMLP, MLPLayer\n",
    "from neural.config import TrainConfig\n",
    "\n",
    "import risk\n",
    "from risk import Risk\n",
    "from algorithm import Hypothesis, ConfSeq\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoffConf(ConfSeq):\n",
    "    def __init__(self, conf_lvl: float, min_val=None, max_val=None):\n",
    "        super().__init__(conf_lvl, min_val, max_val)\n",
    "    \n",
    "    def calculate_cs(self, x):\n",
    "        # min_val, max_val =  x.min(), x.max()\n",
    "        # normalized = (x - min_val) / (max_val - min_val)\n",
    "        lower_cs, higher_cs = predmix.predmix_hoeffding_cs(x, self.conf_lvl)\n",
    "\n",
    "        # lower_cs = lower_cs * (max_val - min_val) + min_val\n",
    "        # higher_cs = higher_cs * (max_val - min_val) + min_val\n",
    "        return lower_cs, higher_cs\n",
    "    \n",
    "    @staticmethod\n",
    "    def standardise(x):\n",
    "        return (x - x.min()) / (x.max() - x.min())\n",
    "    \n",
    "\n",
    "class EmpbernConf(ConfSeq):\n",
    "    def __init__(self, conf_lvl: float, min_val=None, max_val=None):\n",
    "        super().__init__(conf_lvl, min_val, max_val)\n",
    "    \n",
    "    def calculate_cs(self, x):\n",
    "        # min_val, max_val =  x.min(), x.max()\n",
    "        # normalized = (x - min_val) / (max_val - min_val)\n",
    "\n",
    "        lower_cs, higher_cs = predmix.predmix_empbern_twosided_cs(x, self.conf_lvl, running_intersection=False)\n",
    "\n",
    "        # lower_cs = lower_cs * (max_val - min_val) + min_val\n",
    "        # higher_cs = higher_cs * (max_val - min_val) + min_val\n",
    "        return lower_cs, higher_cs\n",
    "    \n",
    "\n",
    "class EmpbernConjmix(ConfSeq):\n",
    "    def __init__(self, conf_lvl: float, min_val=None, max_val=None):\n",
    "        super().__init__(conf_lvl, min_val, max_val)\n",
    "    \n",
    "    def calculate_cs(self, x):\n",
    "        # min_val, max_val =  x.min(), x.max()\n",
    "        # normalized = (x - min_val) / (max_val - min_val)\n",
    "        lower_cs, higher_cs = conjmix_bounded.conjmix_empbern_twosided_cs(x, 1/12, self.conf_lvl)\n",
    "\n",
    "        # lower_cs = lower_cs * (max_val - min_val) + min_val\n",
    "        # higher_cs = higher_cs * (max_val - min_val) + min_val\n",
    "        return lower_cs, higher_cs\n",
    "\n",
    "class EmpbernBetting(ConfSeq):\n",
    "    def __init__(self, conf_lvl: float, min_val=None, max_val=None):\n",
    "        super().__init__(conf_lvl, min_val, max_val)\n",
    "    \n",
    "    def calculate_cs(self, x):\n",
    "        # min_val, max_val =  x.min(), x.max()\n",
    "        # normalized = (x - min_val) / (max_val - min_val)\n",
    "        lower_cs, higher_cs = betting.betting_cs(x, self.conf_lvl)\n",
    "\n",
    "        # lower_cs = lower_cs * (max_val - min_val) + min_val\n",
    "        # higher_cs = higher_cs * (max_val - min_val) + min_val\n",
    "        return lower_cs, higher_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H0(Hypothesis):\n",
    "    def __init__(self, tolerance:float, lower_bound:ConfSeq, upper_bound:ConfSeq):\n",
    "        super().__init__(tolerance, lower_bound, upper_bound)\n",
    "    \n",
    "    @property\n",
    "    def source_upper(self):\n",
    "        return self.source_upper_cs[-1] + self.tolerance\n",
    "    \n",
    "    @property\n",
    "    def target_lower(self):\n",
    "        return self.target_lower_cs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeployedModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: RandomForestRegressor,\n",
    "        hyp_tests: list[tuple[Hypothesis, Risk]]\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.hyp_tests = hyp_tests\n",
    "\n",
    "    def update(self, features:np.ndarray, labels: np.ndarray) -> bool:\n",
    "        preds = self.model.predict(features)\n",
    "        # update targed confidence bounds\n",
    "        test_results = []\n",
    "        for hyp, risk in self.hyp_tests:\n",
    "            res = hyp.test(features, labels, preds, risk)\n",
    "            test_results.append(res)\n",
    "        return all(test_results)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # fit model\n",
    "        self.model.fit(x, y)\n",
    "        # calibrate source confidence bounds\n",
    "        preds = self.model.predict(x)\n",
    "        for hyp, risk in self.hyp_tests:\n",
    "            hyp.calc_source_upper_cs(risk(features=x, preds=preds, labels=y))\n",
    "        \n",
    "    def to_dataframe(self):\n",
    "        results = []\n",
    "        for hyp, risk in self.hyp_tests:\n",
    "            res = hyp.to_dataframe()\n",
    "            res['risk'] = risk.__class__.__name__\n",
    "            res['hyp'] = hyp.__class__.__name__\n",
    "            results.append(res)\n",
    "        return pd.concat(results, axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_ds,\n",
    "        test_ds,\n",
    "        test_batch_size=1,\n",
    "        \n",
    "    ):\n",
    "        self.model: DeployedModel = model\n",
    "        self.train_ds: EnergyDataset = train_ds\n",
    "        self.test_ds: EnergyDataset = test_ds\n",
    "        self.test_batch_size: int = test_batch_size\n",
    "\n",
    "    def train(self):\n",
    "        self.model.fit(self.train_ds.features, self.train_ds.labels)\n",
    "\n",
    "    def test(self):\n",
    "        ds_len = len(self.test_ds)\n",
    "        for i in range(0, ds_len, self.test_batch_size):\n",
    "            # prepare batch\n",
    "            batch_end = max(i+self.test_batch_size, ds_len)\n",
    "            x = self.test_ds.features[i:batch_end]\n",
    "            y = self.test_ds.labels[i:batch_end]\n",
    "            # predict\n",
    "            result = self.model.update(x, y)\n",
    "\n",
    "        return self.model.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Hourly_Energy_Consumption/AEP_hourly.csv\"\n",
    "all_paths = ['Hourly_Energy_Consumption/AEP_hourly.csv',\n",
    "            'Hourly_Energy_Consumption/COMED_hourly.csv',\n",
    "            'Hourly_Energy_Consumption/DAYTON_hourly.csv',\n",
    "            'Hourly_Energy_Consumption/DEOK_hourly.csv',\n",
    "            'Hourly_Energy_Consumption/DOM_hourly.csv',\n",
    "            'Hourly_Energy_Consumption/DUQ_hourly.csv',\n",
    "            'Hourly_Energy_Consumption/EKPC_hourly.csv',\n",
    "            'Hourly_Energy_Consumption/FE_hourly.csv',\n",
    "            'Hourly_Energy_Consumption/NI_hourly.csv',\n",
    "            'Hourly_Energy_Consumption/PJME_hourly.csv',\n",
    "            'Hourly_Energy_Consumption/PJMW_hourly.csv',\n",
    "            # 'Hourly_Energy_Consumption/PJM_Load_hourly.csv',\n",
    "            # 'Hourly_Energy_Consumption/est_hourly.paruqet',\n",
    "            # 'Hourly_Energy_Consumption/pjm_hourly_est.csv'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = DataPrep(*all_paths)\n",
    "data = prep.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = DatasetConfig(\n",
    "    past_window=24,\n",
    "    years=(2006, 2010),\n",
    "    region=REGION.AEP,\n",
    ")\n",
    "test_config = DatasetConfig(\n",
    "    past_window=24,\n",
    "    years=(2010, 2014),\n",
    "    region=REGION.AEP,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = EnergyDataset(path, train_config)\n",
    "test_ds = EnergyDataset(path, test_config)\n",
    "\n",
    "columns = train_ds.features_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_risk = risk.MSE()\n",
    "hyp_test_1 = H0(tolerance=0.1,\n",
    "                lower_bound=EmpbernConf(0.05),\n",
    "                upper_bound=EmpbernConf(0.05)\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeployedModel(rf, [(hyp_test_1, mse_risk)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(model, train_ds, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = experiment.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = TrainConfig(\n",
    "        # loss_fn = 'bce', # str\n",
    "        # optimizer = 'adam', # str\n",
    "        device = f\"gpu\", # str\n",
    "        log = False, # bool\n",
    "        logs_dir = \"logs/\", # str\n",
    "        num_epochs = 30, # int\n",
    "        checkpoints = 'test.pt', # str\n",
    "        early_stopping = None, # int\n",
    "        log_every = 1, # int\n",
    "        timeout = \"00:12:00:00\", # int\n",
    "        # learning_rate = 0.001, # float\n",
    "        # weight_decay = 1e-06, # float\n",
    "        batch_size = 2048, # int\n",
    "        shuffle = False, # bool\n",
    "        num_workers = 9, # int\n",
    "        # train_test_split = 0.5, # float\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Hourly_Energy_Consumption/AEP_hourly.csv\"\n",
    "past_hours = 2\n",
    "\n",
    "prper = DataPrep(path)\n",
    "data = prper.data\n",
    "\n",
    "# train_years = range(2004, 2016)\n",
    "# val_years = range(2016, 2018)\n",
    "\n",
    "def year_cond(start:int, end:int):\n",
    "    return lambda data: data['Year'].between(start, end)\n",
    "\n",
    "def season_cond(season:SEASON):\n",
    "    return lambda data: data['Season'] == season.value\n",
    "\n",
    "def month_cond(month:int):\n",
    "    return lambda data: data['Month'] == month\n",
    "\n",
    "def cond_and(conds:list):\n",
    "    def cond(data):\n",
    "        mask = pd.Series(len(data)*[True])\n",
    "        for c in conds:\n",
    "            mask = mask & c(data)\n",
    "        return mask\n",
    "    # return lambda data: ([cond(data) for cond in conds])\n",
    "    return cond\n",
    "\n",
    "train_conds = [year_cond(2006, 2008), season_cond(SEASON.WINTER)]\n",
    "val_conds = [year_cond(2009, 2012), season_cond(SEASON.SUMMER)]\n",
    "\n",
    "train_ds = EnergyDataset(path, past_hours=past_hours, condition=cond_and(train_conds))\n",
    "val_ds = EnergyDataset(path, past_hours=past_hours, condition=cond_and(val_conds))\n",
    "\n",
    "\n",
    "datamodule = DataModule(train_ds,\n",
    "                        val_ds,\n",
    "                        batch_size=settings.batch_size,\n",
    "                        num_workers=settings.num_workers,\n",
    "                        shuffle=settings.shuffle,\n",
    "                        seed=42,\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datamodule.prepare_data()\n",
    "dl = datamodule.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, label = next(iter(dl))\n",
    "n_features = batch.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.shape, label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class ResidMLP(LightningModel):\n",
    "    def __init__(self, input_size, hidden_size, output_size=1, loss:nn.Module=nn.L1Loss()):\n",
    "        super().__init__(loss)\n",
    "        # self.f = nn.TransformerEncoderLayer(input_size,\n",
    "        #                                     nhead=4,\n",
    "        #                                     dim_feedforward=hidden_size[-1],\n",
    "        #                                     dropout=0.1,\n",
    "        #                                     activation='relu',\n",
    "        #                                     batch_first=True)\n",
    "        \n",
    "        # self.rmlp = RMLP(\n",
    "        #     in_dim=input_size,\n",
    "        #     block_in_dim=hidden_size[0], \n",
    "        #     block_dims=hidden_size,\n",
    "        #     block_nonlins=[nn.ReLU()]*len(hidden_size),\n",
    "        #     n_blocks=2,\n",
    "        #     out_dim=output_size,\n",
    "        #     out_nonlin=nn.Identity(),\n",
    "        #     batch_norm=False,\n",
    "        #     )\n",
    "        # self.mlp = MlpBlock(\n",
    "        #     in_dim=input_size,\n",
    "        #     dims=hidden_size,\n",
    "        #     nonlins=[nn.Tanh()]*len(hidden_size),\n",
    "        #     batch_norm=False,\n",
    "        #     )\n",
    "        self.out_layer = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = x.flatten(1).to(torch.float32)\n",
    "        # x = self.rmlp(x)\n",
    "        # x = self.mlp(x)\n",
    "        # x = self.f(x)\n",
    "        return self.out_layer(x)#.squeeze(-1)\n",
    "    \n",
    "    def accuracy(self, preds: Tensor, labels: Tensor) -> Tensor:\n",
    "        return (preds - labels).abs().mean()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size = n_features#*past_hours\n",
    "rmlp = ResidMLP(input_size=in_size, hidden_size=[24, 16], output_size=1)\n",
    "\n",
    "model = LightningWrapper(rmlp)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = LightningTrainer(settings, \"global_wheat_1\")\n",
    "train_dl, val_dl = datamodule.train_dataloader(), datamodule.val_dataloader()\n",
    "trainer.fit(model, dl_train=train_dl, dl_test=val_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = rf.fit(train_ds.features, train_ds.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(preds, labels):\n",
    "    return np.abs(preds - labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = mae(rf.predict(train_ds.features), train_ds.labels.numpy())\n",
    "val_scores = mae(rf.predict(val_ds.features), val_ds.labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate(model:RandomForestRegressor, val_dl):\n",
    "    scores = []\n",
    "    for batch, label in val_dl:\n",
    "        score = np.abs(model.predict(batch.numpy()) - label.numpy()).flatten()\n",
    "        scores.append(score)\n",
    "    return np.concatenate(scores)\n",
    "\n",
    "train_scores = validate(rf, train_dl)\n",
    "val_scores = validate(rf, val_dl)\n",
    "# scores = []\n",
    "# for batch, label in val_dl:\n",
    "#     score = np.abs(rf.predict(batch.numpy()) - label.numpy()).mean()\n",
    "#     scores.append(score)\n",
    "# print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train_score = {train_scores.mean()}\")\n",
    "print(f\"val_score = {val_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
